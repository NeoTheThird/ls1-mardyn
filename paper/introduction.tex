Molecular dynamics deals with the simulation of atoms and molecules. It estimates the interaction between a high number of molecules and atoms using force fields which are modeled after quantum mechanics equations. 
Such simulations are important for many different areas in biology, chemistry and material sciences. For example you can examine the folding of proteins or the nanoscopic behavior of materials.

The simulations are sped up using sophisticated approximations and algorithms. I have used MarDyn, a molecular dynamics application, for my work. It has been co-developed by the Chair of Scientific Computing at the Technische Universit\"at M\"unchen and a description of it can be found in \cite{buchholz10framework}.

\subsection{Parallelization}
Molecules only have strong interactions with molecules that are nearby. One of the first approximations is to take into account these strong interactions only and ignore weaker long-distance ones.
This space locality of the calculations leads to the Linked Cell-algorithm. It divides the simulation space into a grid of cells. Only interactions between molecules inside each cell and with nearby cells have to be calculated. This reduces, assuming an upper density limit of the molecules,
 the complexity from $ \mathcal{O} \left( N^2 \right) $ to $ \mathcal{O} \left( N \right) $.
Thus different cells that are not neighbors show a data independency which can be used for parallelization.

One way of parallelizing the calculations is using clusters of processing units which is also discussed in \cite{buchholz10framework}. Another option is the parallelization using modern GPUs which offer multiple processing cores and high parallelism in a single computer.

There are two programming frameworks for GPUs: \cuda{}\footnote{\url{http://developer.nvidia.com/category/zone/cuda-zone}} and OpenCL\footnote{\url{http://www.khronos.org/opencl/}}.
\cite{orend10numerische} describes how to parallelize MarDyn on a GPU using OpenCL. I am using \cuda{}.

\subsection{\cuda{}}
In \cuda{} 2.x the GPU is modeled as having multiple \textbf{streaming multiprocessors} (\textbf{SM}) which execute multiple threads of so-called kernels. A \textbf{kernel} is the term used in \cuda{} for a GPU program.

Each kernel can have many \textbf{threads} that are run in parallel. The threads are grouped in \textbf{thread blocks} of at most 1024 threads and launched in a \textbf{grid} of at most $65535^2$ thread blocks.

Each multiprocessor has at most 1536 \textbf{resident threads} which can be resumed with almost no overhead and runs up to 3 thread blocks. That is all threads of one thread block run on the same multiprocessor. 32 threads (belonging to the same thread block and kernel) constitute a warp. A \textbf{warp} is executed in real parallel by the multiprocessor. Internally the multiprocessor uses a principle called \textbf{SIMT} which stands for \textit{Single-Instruction Multiple-Thread}.

This means that the instructions of a GPU program run in lock-step for all threads of a warp. If there is a branch that is taken differently by different threads of a warp, both code paths are executed by the multiprocessor sequentially. Register and memory operations are only \textit{masked out} for the inactive threads.

There are different types of memory: \textbf{global memory} and \textbf{constant memory} are shared between all kernels and thread blocks; \textbf{shared memory} is on-chip memory on a multiprocessor and only shared between threads of a single thread block; \textbf{local memory} and \textbf{registers} only belong to one thread.

There are 64 KB of constant memory available and 48 KB of shared memory available per multiprocessor. Each thread has access to 512 KB of local memory. There are 32768 registers that are divided up between the threads.

Additionally there is a 16 KB big L1 cache in each multiprocessor for global memory. There is a second cache mode in which only 16 KB of shared memory are available and L1 cache is 48 KB big instead.

The register count used by one thread and the amount of shared memory used per thread block determine how many threads and thread blocks can be executed on the same multiprocessor concurrently.

Most importantly \cuda{} uses threads and thread blocks to hide the high latency of memory accesses. Thus it is imperative to keep the multiprocessors busy with a high amount of threads, so a multiprocessor can resume a different warp, while the current one is waiting for a memory access to finish.

For more information see \cite{cuda11progguide} and \cite{cuda11bestpract}.