#include "cutil_math.h"

#define MAX_BLOCK_SIZE 512
#define WARP_SIZE 32
#define NUM_WARPS 4
#define BLOCK_SIZE (WARP_SIZE*NUM_WARPS)

// threadIdx.xy = intraWarpIndex | warpIndex

// = ceil( a / b )
__device__ inline int iceil(int a, int b) {
	return (a+b-1) / b;
}

// = b if a % b = 0, a % b otherwise
__device__ inline int shiftedMod( int a, int b ) {
	int r = a % b;
	return (r > 0) ? r : b;
}

__device__ inline void reducePotentialAndVirial( float *potential, float *virial ) {
	// ASSERT: BLOCK_SIZE is power of 2
	for( int power = 2 ; power <= BLOCK_SIZE ; power <<= 1 ) {
		__syncthreads();

		const int index = WARP_SIZE * threadIdx.y + threadIdx.x;
		if( (index & (power-1)) == 0 ) {
			const int neighborIndex = index + (power >> 1);

			potential[index] += potential[neighborIndex];
			virial[index] += virial[neighborIndex];
		}
	}
}

struct InteractionParameters {
	float cutOffRadiusSquared;
	float epsilon;
	float sigmaSquared;
};

__device__ inline void calculateInteraction(
		const InteractionParameters &parameters,
		const float3 &positionA, const float3 &positionB,
		float3 &forceA, float3 &forceB,
		float &totalPotential, float &totalVirial
	) {
	const float3 distance = positionB - positionA;
	const float distanceSquared = dot( distance, distance );

	if( distanceSquared > parameters.cutOffRadiusSquared ) {
		return;
	}

	float3 force;
	float potential;
	calculateLennardJones( distance, distanceSquared, parameters.epsilon, parameters.sigmaSquared, force, potential );

	totalPotential += potential;
	float virial = dot( force, distance );
	totalVirial += virial;

	forceA += force;
	forceB -= force;
}

template< bool sameBlock >
__device__ inline void processBlock(
		const int indexA,
		const int numCellsInBlockA,
		const int numCellsInBlockB,
		const InteractionParameters &interactionParameters,
		const float3 &positionA, const float3 *positionsB,
		float3 &forceA, float3 *forcesB,
		float &totalPotential, float &totalVirial
		) {
	__shared__ float3 cachedBForces[BLOCK_SIZE];
	__shared__ float3 cachedBPositions[BLOCK_SIZE];

	// load B data into cache
	if( indexA < numCellsInBlockB ) {
		cachedBForces[indexA] = forcesB[indexA];
		cachedBPositions[indexA] = positionsB[indexA];
	}

	// I'm working on WARP_SIZE many data entries at once during processing, so there should be a natural synchronization?
	// TODO: remove this __syncthreads maybe?
	__syncthreads();

	// process block
	const int numShifts = max( numCellsInBlockA, numCellsInBlockB );
	const int numWarps = iceil( numShifts, WARP_SIZE );
	const int numWarpsInBlockB = iceil( numCellsInBlockB, WARP_SIZE );

	for( int warpShiftIndex = 0 ; warpShiftIndex < numWarps ; warpShiftIndex++ ) {
		int warpIndexB = (threadIdx.y + warpShiftIndex) % numWarps;

		if( indexA < numCellsInBlockA && warpIndexB < numWarpsInBlockB ) {
			const int numCellShifts = WARP_SIZE;

			for( int cellShiftIndex = 0 ; cellShiftIndex < numCellShifts ; cellShiftIndex++ ) {

				const int indexB = warpIndexB * WARP_SIZE + ((threadIdx.x + cellShiftIndex) % WARP_SIZE);
				if( indexB >= numCellsInBlockB ) {
					continue;
				}

				// if we're not inside the same warp, process all WARP_SIZE * WARP_SIZE pairs inside a warp
				// otherwise only process the lower half
				if( sameBlock && indexA >= indexB ) {
					continue;
				}

				calculateInteraction( interactionParameters,
						positionA, cachedBPositions[indexB],
						forceA, cachedBForces[indexB],
						totalPotential, totalVirial
					);
			}
		}
		// sync all warps, so that no two different warps will try to access the same warp data block
		__syncthreads();
	}

	// push B data back
	if( indexA < numCellsInBlockB ) {
		forcesB[indexA] = cachedBForces[indexA];
	}
}

__global__ void Kernel_calculatePairLJForces( float3 *positions, OUT float3 *forces, int2 *cellInfos, OUT float2 *domainValues,
		int startIndex, int2 dimension, int3 gridOffsets,
		int neighborOffset,
		float epsilon, float sigmaSquared, float cutOffRadiusSquared ) {
	InteractionParameters parameters;
	parameters.cutOffRadiusSquared = cutOffRadiusSquared;
	parameters.epsilon = epsilon;
	parameters.sigmaSquared = sigmaSquared;

	int cellIndex = getCellIndex( startIndex, dimension, gridOffsets );
	int neighborIndex = cellIndex + neighborOffset;

	// ensure that cellA_length <= cellB_length (which will use fewer data transfers)
	// (numTransfersA + numTransfersA * numTransfersB) * BLOCK_SIZE
	if( cellInfos[cellIndex].y > cellInfos[neighborIndex].y ) {
		// swap cellIndex and neighborIndex
		cellIndex = neighborIndex;
		neighborIndex -= neighborOffset;
	}

	const int cellAStart = cellInfos[cellIndex].x;
	const int cellALength = cellInfos[cellIndex].y;
	const int cellBStart = cellInfos[neighborIndex].x;
	const int cellBLength = cellInfos[neighborIndex].y;

	__shared__ float totalThreadPotential[BLOCK_SIZE];
	__shared__ float totalThreadVirial[BLOCK_SIZE];

	const int indexA = threadIdx.y * WARP_SIZE + threadIdx.x;

	totalThreadPotential[indexA] = 0.0f;
	totalThreadVirial[indexA] = 0.0f;

	__syncthreads();

	const int numBlocksA = iceil( cellALength, BLOCK_SIZE );
	const int numBlocksB = iceil( cellBLength, BLOCK_SIZE );
	for( int blockIndexA = 0 ; blockIndexA < numBlocksA ; blockIndexA++ ) {
		float3 cachedAForce;
		float3 cachedAPosition;

		const int numCellsInBlockA = (blockIndexA < numBlocksA - 1) ? BLOCK_SIZE : shiftedMod( cellALength, BLOCK_SIZE );
		const int blockOffsetA = cellAStart + blockIndexA * BLOCK_SIZE;

		// load A data into (register) cache
		if( indexA < numCellsInBlockA ) {
			cachedAForce = forces[blockOffsetA + indexA];
			cachedAPosition = positions[blockOffsetA + indexA];
		}

		for( int blockIndexB = 0 ; blockIndexB < numBlocksB ; blockIndexB++ ) {
			const int numCellsInBlockB = (blockIndexB < numBlocksB - 1) ? BLOCK_SIZE : shiftedMod( cellBLength, BLOCK_SIZE );

			const int blockOffsetB = cellBStart + blockIndexB * BLOCK_SIZE;

			processBlock<false>(
					indexA,
					numCellsInBlockA,
					numCellsInBlockB,
					parameters,
					cachedAPosition, positions + blockOffsetB,
					cachedAForce, forces + blockOffsetB,
					totalThreadPotential[indexA], totalThreadVirial[indexA]
				);
		}

		// push A data back
		if( indexA < numCellsInBlockA ) {
			forces[blockOffsetA + indexA] = cachedAForce;
		}
	}

	// reduce the potential and the virial
	// ASSERT: BLOCK_SIZE is power of 2
	reducePotentialAndVirial( totalThreadPotential, totalThreadVirial );

	if( threadIdx.x == 0 && threadIdx.y == 0 ) {
		domainValues[cellIndex].x += totalThreadPotential[0];
		domainValues[cellIndex].y += totalThreadVirial[0];
		domainValues[neighborIndex].x += totalThreadPotential[0];
		domainValues[neighborIndex].y += totalThreadVirial[0];
	}
}

__global__ void Kernel_calculateInnerLJForces( float3 *positions, OUT float3 *forces, int2 *cellInfos, OUT float2 *domainValues,
		float epsilon, float sigmaSquared, float cutOffRadiusSquared ) {
	InteractionParameters parameters;
	parameters.cutOffRadiusSquared = cutOffRadiusSquared;
	parameters.epsilon = epsilon;
	parameters.sigmaSquared = sigmaSquared;

	const int cellIndex = blockIdx.x;

	const int cellStart = cellInfos[cellIndex].x;
	const int cellLength = cellInfos[cellIndex].y;

	__shared__ float totalThreadPotential[BLOCK_SIZE];
	__shared__ float totalThreadVirial[BLOCK_SIZE];

	const int indexA = threadIdx.y * WARP_SIZE + threadIdx.x;

	totalThreadPotential[indexA] = 0.0f;
	totalThreadVirial[indexA] = 0.0f;

	const int numBlocks = iceil( cellLength, BLOCK_SIZE );
	for( int blockIndexA = 0 ; blockIndexA < numBlocks ; blockIndexA++ ) {
		float3 cachedAForce;
		float3 cachedAPosition;

		const int numCellsInBlockA = (blockIndexA < numBlocks - 1) ? BLOCK_SIZE : shiftedMod( cellLength, BLOCK_SIZE );
		const int blockOffsetA = cellStart + blockIndexA * BLOCK_SIZE;

		// load A data into (register) cache
		if( indexA < numCellsInBlockA ) {
			cachedAForce = make_float3( 0.0f );
			cachedAPosition = positions[blockOffsetA + indexA];
		}

		processBlock<true>(
				indexA,
				numCellsInBlockA,
				numCellsInBlockA,
				parameters,
				cachedAPosition, positions + blockOffsetA,
				cachedAForce, forces + blockOffsetA,
				totalThreadPotential[indexA], totalThreadVirial[indexA]
			);

		for( int blockIndexB = 0 ; blockIndexB < blockIndexA ; blockIndexB++ ) {
			const int numCellsInBlockB = (blockIndexB < numBlocks - 1) ? BLOCK_SIZE : shiftedMod( cellLength, BLOCK_SIZE );

			const int blockOffsetB = cellStart + blockIndexB * BLOCK_SIZE;

			processBlock<false>(
					indexA,
					numCellsInBlockA,
					numCellsInBlockB,
					parameters,
					cachedAPosition, positions + blockOffsetB,
					cachedAForce, forces + blockOffsetB,
					totalThreadPotential[indexA], totalThreadVirial[indexA]
				);
		}

		// push A data back
		if( indexA < numCellsInBlockA ) {
			forces[blockOffsetA + indexA] += cachedAForce;
		}
	}

	/// reduce the potential and the virial
	// ASSERT: BLOCK_SIZE is power of 2
	reducePotentialAndVirial( totalThreadPotential, totalThreadVirial );

	if( threadIdx.x == 0 && threadIdx.y == 0 ) {
		domainValues[cellIndex].x = totalThreadPotential[0] * 2;
		domainValues[cellIndex].y = totalThreadVirial[0] * 2;
	}
}
