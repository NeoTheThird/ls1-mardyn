#ifndef GLOBALSTATS_CUM__
#define GLOBALSTATS_CUM__

#include "sharedDecls.h"

#include "locks.cum"

struct CellStats : CellStatsStorage {
	__device__ void add( floatType potential, floatType virial ) {
		this->potential += potential;
		this->virial += virial;
	}
};

// should be: __device__ CellStats cellStats[CellCount]
__constant__ __device__ CellStats * cellStats;

#ifdef CUDA_WARP_BLOCK_CELL_PROCESSOR
__constant__ Lock::Mutex *cellLocks;
#endif

// TODO: rename BlockStatsCollector
template<int blockSize>
struct CellStatsCollector {
	floatType threadPotential[blockSize];
	floatType threadVirial[blockSize];

	__device__ void initThreadLocal(int threadIndex) {
		threadPotential[threadIndex] = 0.0f;
		threadVirial[threadIndex] = 0.0f;

		// no synchronization required because each thread only accesses its own entry
	}

	__device__ void add(int threadIndex, floatType potential, floatType virial) {
		threadPotential[threadIndex] += potential;
		threadVirial[threadIndex] += virial;
	}

	__device__ void reduceWarps(int threadIndex) {
#if WARP_SIZE > 1
		// no synchronization required since every warp is automatically synchronized
#	if 1
#		if WARP_SIZE != 32
#			error WARP_SIZE == 32 assumed
#		endif

#		pragma unroll 5
		for( int index = 4 ; index >= 0 ; index-- ) {
			const int shift = 1 << index;
			if( threadIdx.x < shift ) {
				const int neighborIndex = threadIndex + shift;
				threadPotential[threadIndex] += threadPotential[neighborIndex];
				threadVirial[threadIndex] += threadVirial[neighborIndex];
			}
		}
#	else
		// this version cannot be unrolled since nvcc is too stupid to analyze the counter >_>
		for( int shift = WARP_SIZE / 2 ; shift >= 1 ; shift >>= 1 ) {
			if( threadIdx.x < shift ) {
				const int neighborIndex = threadIndex + shift;
				threadPotential[threadIndex] += threadPotential[neighborIndex];
				threadVirial[threadIndex] += threadVirial[neighborIndex];
			}
		}
#	endif
#endif
	}

	// TODO: rename Save to Store
	__device__ __noinline__ void reduceAndStore(int threadIndex, int cellIndexA, int cellIndexB) {
		reduceWarps( threadIndex );

		__syncthreads();

		if( threadIndex == 0 ) {
			floatType potential = 0.0;
			floatType virial = 0.0;

			for( int warpIndex = 0 ; warpIndex < (blockSize / WARP_SIZE) ; warpIndex++ ) {
				potential += threadPotential[ warpIndex * WARP_SIZE ];
				virial += threadVirial[ warpIndex * WARP_SIZE ];
			}

			cellStats[cellIndexA].add( potential, virial );
			cellStats[cellIndexB].add( potential, virial );
		}
	}

#ifdef CUDA_WARP_BLOCK_CELL_PROCESSOR
	__device__ __noinline__ void reduceAndStoreWarp(int threadIndex, int cellIndex) {
		// no synchronization required since every warp is automatically synchronized
		reduceWarps( threadIndex );

		if( threadIdx.x == 0 ) {
			floatType potential = threadPotential[ threadIndex ];
			floatType virial = threadVirial[ threadIndex ];

			cellLocks[cellIndex].lock();
			WARP_PRINTF( "%i cell lock entered\n", cellIndex );

			WARP_PRINTF( "%i: old %f; %f\n", cellIndex, cellStats[cellIndex].potential, potential );
			cellStats[cellIndex].add( potential, virial );

			//__threadfence();
			WARP_PRINTF( "%i: new %f\n", cellIndex, cellStats[cellIndex].potential );
			__threadfence();
			WARP_PRINTF( "%i cell lock left\n", cellIndex );
			cellLocks[cellIndex].unlock();
		}
	}
#endif
};

#endif
