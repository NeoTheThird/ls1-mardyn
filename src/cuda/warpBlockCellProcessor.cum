#ifndef WARPBLOCKCELLPROCESSOR_CUM__
#define WARPBLOCKCELLPROCESSOR_CUM__

#include <host_defines.h>

#include "util.cum"

#include "cellInfo.cum"
#include "moleculeStorage.cum"
#include "locks.cum"

#define WBM_QUEUE_SIZE 4

// TODO: rename to WBCP
namespace WarpBlockMode {

struct WarpBlockInfo {
	CellInfo cell;
	uint cellIndex;
	uint warpBlockIndex;

	__device__ WarpBlockInfo() {}
	__device__ WarpBlockInfo( const CellInfo cell, uint cellIndex, uint warpBlockIndex ) : cell( cell ), cellIndex( cellIndex ), warpBlockIndex( warpBlockIndex ) {}
};

struct WarpBlockPairInfo {
	WarpBlockInfo warpBlockA;
	CellInfo cellB;

	__device__ WarpBlockPairInfo() {}
	__device__ WarpBlockPairInfo( WarpBlockInfo warpBlockA, CellInfo  cellB ) : warpBlockA( warpBlockA ), cellB( cellB ) {}
	__device__ WarpBlockPairInfo( const WarpBlockPairInfo &other ) : warpBlockA( other.warpBlockA ), cellB( other.cellB ) {}
};

/*
 * lock-free queue, assumes one consumer and one producer
 */
template<typename ItemType, uint maxSize>
class RingQueue {
private:
	ItemType buffer[maxSize];
	volatile uint readIndex;
	volatile uint writeIndex;

public:
	__device__ RingQueue() : readIndex( 0 ), writeIndex( 0 ) {}

	__device__ bool isEmpty()  {
		return readIndex == writeIndex;
	}

	__device__ bool isFull() {
		return writeIndex >= readIndex + maxSize;
	}

	__device__ uint getCapacity() {
		return readIndex + maxSize - writeIndex;
	}

	__device__ void push(const ItemType & item) {
		// assert: !isFull
		buffer[writeIndex++ % maxSize] = item;
	}

	__device__ ItemType pop() {
		return buffer[readIndex++ % maxSize];
	}
};

template< uint warpBlockCount, typename WarpJobDescriptor >
struct ThreadBlockInfo { // TODO: rename ThreadBlockInfo
	RingQueue<WarpJobDescriptor, WBM_QUEUE_SIZE> warpJobQueue[warpBlockCount];

	// bitmask for
	volatile uint jobIdleSignal;
	volatile bool hasMoreJobs;

	// global lock
	__device__ ThreadBlockInfo() : hasMoreJobs( true ), jobIdleSignal( 0 ) {}
};

template<uint warpBlockCount, typename WarpJobDescriptor, template<uint> class SpecificScheduler>
class GenericScheduler {
private:
	Lock::Mutex globalMutex;

public:
#if 1
#warning semaphore scheduler
	// returns false if the specific warp block should terminate
		__device__ __noinline__ void scheduleWarpBlocks( ThreadBlockInfo<warpBlockCount, WarpJobDescriptor> & __restrict__ threadBlockInfo ) {
			const uint warpBit = 1 << threadIdx.y;

			// only thread 0 of the current warp runs here
			if( threadIdx.x == 0 ) {
				// signal that this warp is idle right now
				atomicOr( (uint*) &threadBlockInfo.jobIdleSignal, warpBit );

				WARP_PRINTF( "globalMutex entering\n" );

				// wait until it's our turn for the lock
				bool haveLock = globalMutex.lock( (uint*) &threadBlockInfo.jobIdleSignal, warpBit );

				// if I dont have the lock the signal has been reset
				if( haveLock ) {
					WARP_PRINTF( "globalMutex lock\n" );

					if( threadBlockInfo.jobIdleSignal ) {
						// create a local copy of warpWaiting
						const uint snapshoptJobIdleSignal = threadBlockInfo.jobIdleSignal;

						assignJobs( threadBlockInfo, snapshoptJobIdleSignal );

						WARP_PRINTF( "jobs assigned\n" );

						__threadfence_block();

						// reset the assigned warpWaiting bits
						// this is the semaphore signal for the other warps in the thread block
						atomicXor( (uint*) &threadBlockInfo.jobIdleSignal, snapshoptJobIdleSignal );
					}
					else {
						WARP_PRINTF( "no idle flags\n" );
					}

					WARP_PRINTF( "globalMutex unlock\n" );

					// release our global lock
					globalMutex.unlock();
				}
				else {
					//WARP_PRINTF( "globalSemaphore signaled\n" );
				}
			}
		}
#else
	// safe implementation

	// returns false if the specific warp block should terminate
	__device__ __noinline__ void scheduleWarpBlocks( ThreadBlockInfo<warpBlockCount, WarpJobDescriptor> & __restrict__ warpInfos ) {
		// only thread 0 of the current warp runs here
		if( threadIdx.x == 0 ) {
			// wait until it's our turn for the lock
			globalMutex.lock();

			WARP_PRINTF( "globalMutex enter\n" );

			// assign jobs
			assignJobs( warpInfos, threadIdx.y );

			WARP_PRINTF( "globalMutex leave\n" );
			__threadfence();

			// release our global lock
			// no sync needed since we are the only ones to access it
			globalMutex.unlock();
		}
	}
#endif


	__device__ void assignJobs( ThreadBlockInfo<warpBlockCount, WarpJobDescriptor> & __restrict__ threadBlockInfo, uint idleJobMask ) {
		const uint idleWarpCount = __popc( idleJobMask );
		//WARP_PRINTF( "assigning jobs for %i warps\n", idleWarpCount );

		for( uint warpJobs = 0 ; warpJobs <  WBM_QUEUE_SIZE ; warpJobs++ ) {
			uint warpMask = idleJobMask;
			for( uint warpCount = 0 ; warpCount < idleWarpCount ; warpCount++ ) {
				int warpIndex = __ffs( warpMask ) - 1;

				// assign jobs
				if( !reinterpret_cast<SpecificScheduler<warpBlockCount> *>(this)->nextWarpBlock() ) {
					threadBlockInfo.hasMoreJobs = false;
					return;
				}

				const WarpJobDescriptor job = reinterpret_cast<SpecificScheduler<warpBlockCount> *>(this)->getWarpBlock();
				threadBlockInfo.warpJobQueue[warpIndex].push( job );

				//WARP_PRINTF("assignJobs: %i, %i\n", warpInfos.warps[warpIndex].warpBlockA.cellIndex, warpInfos.warps[warpIndex].warpBlockA.warpBlockIndex );

				// remove the updated warp bit from warpIndex
				warpMask ^= 1 << warpIndex;
			}
		}
	}
};

template<uint warpBlockCount>
class CellScheduler : public GenericScheduler<warpBlockCount, WarpBlockPairInfo, CellScheduler> {
private:
	int jobIndex;

	int cellIndex;
	CellInfoEx cell;
	uint numWarpBlocks;
	uint warpBlockIndex;

public:
	__device__ WarpBlockPairInfo getWarpBlock() {
		return WarpBlockPairInfo( WarpBlockInfo( cell, cellIndex, warpBlockIndex ), cell );
	}

	__device__ bool nextWarpBlock() {
		if( ++warpBlockIndex < numWarpBlocks ) {
			return true;
		}

		do {
			if( ++jobIndex >= PairTraverser::numJobs ) {
				return false;
			}

			cellIndex = PairTraverser::getInnerCellIndexFromJobIndex(jobIndex);
			cell = CellInfoEx( cellInfoFromCellIndex( cellIndex ) );
		}
		while( cell.length == 0 );

		numWarpBlocks = iceil( cell.length, WARP_SIZE );
		warpBlockIndex = 0;

		//WARP_PRINTF( "nextWarpBlock: %i\n", cellIndex );
		return true;
	}

public:
	__device__ CellScheduler() : jobIndex( -1 ), cellIndex( 0 ), cell( CellInfo( 0, 0 ) ), numWarpBlocks( 0 ), warpBlockIndex( 0 ) {}
};

template<uint warpBlockCount>
class CellPairScheduler : public GenericScheduler<warpBlockCount, WarpBlockPairInfo, CellPairScheduler> {
private:
	int jobIndex;

	int cellIndexA;
	CellInfoEx cellA;
	uint numWarpBlocksA;
	uint warpBlockIndexA;

	CellInfoEx cellB;

public:
	__device__ WarpBlockPairInfo getWarpBlock() {
		return WarpBlockPairInfo( WarpBlockInfo( cellA, cellIndexA, warpBlockIndexA ), cellB );
	}

	__device__ bool nextWarpBlock() {
		if( ++warpBlockIndexA < numWarpBlocksA) {
			return true;
		}

		do {
			jobIndex++;

			if( jobIndex >= 2 * PairTraverser::numJobs ) {
				return false;
			}

			const int originalCellIndex = PairTraverser::getCellIndexFromJobIndex( jobIndex % PairTraverser::numJobs );
			const int neighborCellIndex = PairTraverser::getNeighborCellIndex( originalCellIndex );

			if( jobIndex < PairTraverser::numJobs ) {
				cellIndexA = originalCellIndex;
				cellA = CellInfoEx( cellInfoFromCellIndex( originalCellIndex) );
				cellB = CellInfoEx( cellInfoFromCellIndex( neighborCellIndex ) );
			}
			else {
				cellIndexA = neighborCellIndex;
				cellA = CellInfoEx( cellInfoFromCellIndex( neighborCellIndex ) );
				cellB = CellInfoEx( cellInfoFromCellIndex( originalCellIndex ) );
			}
		}
		while( cellA.length == 0 || cellB.length == 0 );

		numWarpBlocksA = iceil( cellA.length, WARP_SIZE );
		warpBlockIndexA = 0;
		return true;
	}

public:
	__device__ CellPairScheduler()
		: jobIndex( -1 ), cellA( CellInfo( 0, 0 ) ), cellB( CellInfo( 0, 0 ) ),
		  cellIndexA( 0 ), numWarpBlocksA( 0 ), warpBlockIndexA( 0 ) {}
};

#if 0
// works correctly lock-free assuming there is exactly one consumer and one producer
template<uint size>
class WarpJobManager {
private:
	uint scheduledJobsCount;
	uint finishedJobsCount;

	bool terminate;

	// TODO: make this non-interleaved
	WarpJob jobs[size];

public:
	void startJob() {
		while( scheduledJobsCount <= finishedJobsCount ) {
			; // spin
		}
	}

	WarpJob getJob() const {
		return jobs[ finishedJobsCount /* + 1 - 1 */ % size ];
	}

	void finishJob() {
		finishedJobsCount++;
	}

	bool addJob( WarpJob job ) {
		if( finishedJobsCount + size > scheduledJobsCount ) {
			jobs[ scheduledJobsCount % size ] = job;
			scheduledJobsCount++;

			return true;
		}
		return false;
	}
};
#endif

template<uint warpBlockCount,
class MoleculeStorage, MoleculeStorage &moleculeStorage,
class MoleculePairHandler, MoleculePairHandler &moleculePairHandler>
struct CellProcessor {
	typedef Molecule<MoleculeStorage, moleculeStorage> StorageMolecule;

	// unified for cells and cell pairs
	__device__ __noinline__ void processCellPair( WarpBlockPairInfo warpBlockPairInfo ) {
		WARP_PRINTF( "processing cell %i block %i/%i and molecules %i - %i\n", warpBlockPairInfo.warpBlockA.cellIndex, warpBlockPairInfo.warpBlockA.warpBlockIndex, iceil( warpBlockPairInfo.warpBlockA.cell.endIndex - warpBlockPairInfo.warpBlockA.cell.startIndex, WARP_SIZE), warpBlockPairInfo.cellB.startIndex, warpBlockPairInfo.cellB.endIndex );

		WarpBlockInfo warpBlockA = warpBlockPairInfo.warpBlockA;

		const uint indexA = warpBlockA.cell.startIndex + warpBlockA.warpBlockIndex * WARP_SIZE + threadIdx.x;
		if( indexA >= warpBlockA.cell.endIndex ) {
			return;
		}

		StorageMolecule moleculeA(indexA);

		// now loop over all molecules in this cell
		CellInfo cellB = warpBlockPairInfo.cellB;

#ifdef CUDA_HW_CACHE_ONLY
		for( int indexB = cellB.startIndex ; indexB < cellB.endIndex ; indexB++ ) {
			StorageMolecule moleculeB(indexB);

			if( indexA != indexB ) {
				moleculePairHandler.process( threadIdx.x + threadIdx.y * WARP_SIZE, moleculeA, moleculeB );
			}
		}
#else
		for( int indexB = cellB.startIndex ; indexB < cellB.endIndex ; indexB++ ) {
			StorageMolecule moleculeB(indexB);

			if( indexA != indexB ) {
				moleculePairHandler.process( threadIdx.x + threadIdx.y * WARP_SIZE, moleculeA, moleculeB );
			}
		}
#endif

		moleculeA.store();

	}
};

}

#endif
